{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c6c327-66bc-4fed-a4b0-6a0ade5a57c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c646ca05-a3bc-4cf2-8e61-517f5c8ff3d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013c0cd7-6d68-4930-ae55-38b8c6f20b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Crear los dirctorios\n",
    "mkdir -p ../../data/{bronze,silver,gold}\n",
    "ls -lrt ../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd856443-5911-4264-9924-13d65c9ea005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imprimir el nombre del ussuario\n",
    "dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43319ef-6685-48e1-ba85-70a9f66457e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 1: Imports y SparkSession\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import json, random, uuid, time\n",
    "from datetime import datetime\n",
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, expr, floor, rand, to_timestamp,\n",
    "    date_format, year, month, dayofmonth, hour, minute, second,\n",
    "    struct, to_json\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, DoubleType, StructType, StructField,\n",
    "    TimestampType\n",
    ")\n",
    "from shapely.geometry import Point, shape\n",
    "\n",
    "# Inicia Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"01_setup_load_sparkified\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Carga configuración\n",
    "cfg      = json.loads(Path(\"/Workspace/Users/santiagobustosp@gmail.com/medellin-bigdata-poc/notebooks/1_simulation/sim_config.json\").read_text())\n",
=======
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import pyarrow\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "cfg      = json.loads(Path(\"/Workspace/Users/danielale22rojas@gmail.com/medellin-bigdata-poc/notebooks/1_Ingestion/sim_config.json\").read_text()) # Cambiar el nombre de ususario antes de ejecutar\n",
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
    "base     = Path(cfg[\"base_path\"])\n",
    "paths    = cfg[\"paths\"]\n",
    "interval = cfg[\"interval_seconds\"]\n",
    "qty_min, qty_max = cfg[\"quantity_range\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3314a0d-1f97-4059-8728-b86bc0c27757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 2: Leer insumos (GeoPandas + Spark)\n",
    "# ==============================\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "# 1) Ruta local al repo (GeoPandas sí puede acceder)\n",
    "raw_dir = \"/Workspace/Users/santiagobustosp@gmail.com/medellin-bigdata-poc/data/raw\"\n",
    "\n",
    "# 2) Leer barrios y máscara con GeoPandas\n",
    "gdf_neigh = gpd.read_parquet(f\"{raw_dir}/medellin_neighborhoods.parquet\")\n",
    "gdf_mask  = gpd.read_parquet(f\"{raw_dir}/50001.parquet\")\n",
    "\n",
    "# 3) Convertir a listas para UDF espacial\n",
    "neigh_list = gdf_neigh.to_dict(\"records\")\n",
    "mask_geom  = shape(gdf_mask.loc[0, \"geometry\"])\n",
    "\n",
    "# 4) Leer clientes y empleados con Spark (Spark sí puede acceder si están en DBFS o S3, pero no en /Workspace)\n",
    "# Solución: también leerlos con pandas si están en /Workspace\n",
    "pdf_cust = pd.read_parquet(f\"{raw_dir}/customers.parquet\")\n",
    "pdf_emp  = pd.read_parquet(f\"{raw_dir}/employees.parquet\")\n",
    "\n",
    "# 5) Convertir a Spark\n",
    "cust_df = spark.createDataFrame(pdf_cust)\n",
    "emp_df  = spark.createDataFrame(pdf_emp)\n",
    "\n",
    "print(f\"✅ Barrios: {len(neigh_list)} | Clientes: {cust_df.count()} | Empleados: {emp_df.count()}\")\n"
=======
    "# Como usamos una distribución gausiana truncada, debemos definir los parametros \n",
    "# Parámetros de la distribución truncada\n",
    "mu = 25      # número promedio de compras por simulación\n",
    "sigma = 10   # dispersión\n",
    "min_val = qty_min  # mínimo número de compras por intervalo\n",
    "max_val = qty_max # máximo aceptado\n",
    "\n",
    "# Convertir a valores estandarizados\n",
    "min_val = (min_val - mu) / sigma\n",
    "max_val = (max_val - mu) / sigma\n",
    "\n",
    "def gen_event():\n",
    "    b  = gdf_neigh.sample(1).iloc[0]\n",
    "    pt = sample_point(b.geometry)\n",
    "    return {\n",
    "      \"order_id\":          str(uuid.uuid4()),\n",
    "      \"date\":              datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "      \"customer_id\":       int(df_cust.customer_id.sample(1).iloc[0]),\n",
    "      \"employee_id\":       int(df_emp.employee_id.sample(1).iloc[0]),\n",
    "      \"quantity_products\": int(truncnorm(min_val, max_val, loc=mu, scale=sigma).rvs()),\n",
    "      \"latitude\":          pt.y,\n",
    "      \"longitude\":         pt.x,\n",
    "      \"neighborhood\":      b[\"IDENTIFICACION\"]\n",
    "    }\n"
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2002ee18-2ccf-45d4-9411-81837e6e074e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "# ==============================\n",
    "# Sección 3: Simulación de eventos con estructura final deseada\n",
    "# ==============================\n",
=======
    "# PASO 4: Preparar carpeta timestamp\n",
    "ts      = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = base/paths[\"output_dir\"]/ts\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"▶️ Carpeta de simulación:\", out_dir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b190138-38f1-46c4-9882-ee2e89e1dad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 5 optimizada para prueba rápida:\n",
    "# Defninimos el número de eventos a generar como una variable aleatoria con distribución normal truncada\n",
    "\n",
    "# Parámetros de la distribución truncada\n",
    "mu = 8      # número promedio de compras por simulación\n",
    "sigma = 5   # dispersión\n",
    "min_val = 0  # mínimo número de compras por intervalo\n",
    "max_val = 30 # máximo aceptado\n",
    "\n",
    "# Convertir a valores estandarizados\n",
    "a = (min_val - mu) / sigma\n",
    "b = (max_val - mu) / sigma\n",
    "N = int(truncnorm(a, b, loc=mu, scale=sigma).rvs())\n",
    "\n",
    "# Corremos el bucle para generar los eventos \n",
    "for _ in range(N):\n",
    "    e = gen_event()\n",
    "    (out_dir/f\"{e['order_id']}.json\").write_text(json.dumps(e))\n",
    "print(f\"✅ Generados {N} eventos en {out_dir.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46aaf3d4-ef59-4565-83ec-00bdf4095f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 6: Leer los JSONs y crear DataFrame Spark\n",
    "files   = list(out_dir.glob(\"*.json\"))\n",
    "events  = [json.loads(p.read_text()) for p in files]\n",
    "df_raw  = spark.createDataFrame(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf716f6-7e66-48ec-a715-7f09ca6d3a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cec9aed-181d-4bee-8851-08b11ae3df91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspección rápida\n",
    "# display(df_raw)\n",
    "df_raw.printSchema()\n",
    "print(\"Total registros:\", df_raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b902eefd-ac2e-4046-90f3-0c485e112868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 7: Geo‑join para calcular 'district'  \n",
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
    "from shapely.geometry import Point\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Número de eventos a generar\n",
    "N = 20\n",
    "\n",
    "# IDs preparados para sampling\n",
    "cust_ids = pdf_cust[\"customer_id\"].tolist()\n",
    "emp_ids  = pdf_emp[\"employee_id\"].tolist()\n",
    "\n",
    "# Generar lista de eventos con estructura exacta\n",
    "events = []\n",
    "for _ in range(N):\n",
    "    b = random.choice(neigh_list)\n",
    "    minx, miny, maxx, maxy = shape(b[\"geometry\"]).bounds\n",
    "    while True:\n",
    "        lon = random.uniform(minx, maxx)\n",
    "        lat = random.uniform(miny, maxy)\n",
    "        pt = Point(lon, lat)\n",
    "        if shape(b[\"geometry\"]).contains(pt) and mask_geom.contains(pt):\n",
    "            break\n",
    "    event = OrderedDict([\n",
    "        (\"latitude\",           lat),\n",
    "        (\"longitude\",          lon),\n",
    "        (\"date\",               datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")),\n",
    "        (\"customer_id\",        random.choice(cust_ids)),\n",
    "        (\"employee_id\",        random.choice(emp_ids)),\n",
    "        (\"quantity_products\",  random.randint(qty_min, qty_max)),\n",
    "        (\"order_id\",           str(uuid.uuid4()))\n",
    "    ])\n",
    "    events.append(event)\n",
    "\n",
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "# Crear DataFrame Spark desde los eventos\n",
    "schema_ev = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), False),\n",
    "    StructField(\"longitude\", DoubleType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"employee_id\", IntegerType(), False),\n",
    "    StructField(\"quantity_products\", IntegerType(), False),\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "])\n",
    "df_raw = spark.createDataFrame(events, schema=schema_ev)\n",
    "print(f\"✅ Generados {df_raw.count()} eventos con estructura correcta\")\n",
    "df_raw.show(5, truncate=False)\n"
=======
    "# 4) Renombrar la columna resultante y limpiar índices\n",
    "# gpdf = gpdf.rename(columns={\"IDENTIFICACION\": \"district\", \"NOMBRE\": \"neighborhood\"}).drop(columns=[\"index_right\"])\n",
    "gpdf = gpdf.rename(columns={\"IDENTIFICACION\": \"neighborhood\", \"NOMBRE\": \"district\"}).drop(columns=[\"index_right\"])\n",
    "\n",
    "\n",
    "# 5) Volver a Spark\n",
    "df_raw = spark.createDataFrame(gpdf.drop(columns=\"geometry\"))\n"
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76eac53-c3dd-4c0d-8cc8-068762b83f18",
     "showTitle": false,
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
     "tableResultSettingsMap": {},
=======
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"date\":138},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753221519306}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
     "title": ""
    }
   },
   "outputs": [],
   "source": [
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "# ==============================\n",
    "# Sección 3.1: Registrar cada evento simulado en su propio JSON\n",
    "# ==============================\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) Timestamp único para esta corrida\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 2) Carpeta destino en tu repo\n",
    "#    base es Path(cfg[\"base_path\"]) → \"/Workspace/Users/.../medellin-bigdata-poc\"\n",
    "output_base = Path(base, \"data\", \"sim-events\", run_ts)\n",
    "\n",
    "# 3) Creo la carpeta (no sobreescribe ejecuciones anteriores)\n",
    "output_base.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "# 4) Escribo cada evento en un archivo JSON separado\n",
    "for ev in events:\n",
    "    order_id = ev[\"order_id\"]\n",
    "    file_path = output_base / f\"{order_id}.json\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ev, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ {len(events)} archivos JSON de eventos escritos en: {output_base}\")\n"
=======
    "# PASO 7: Inspección rápida\n",
    "# display(df_raw)\n",
    "df_raw.printSchema()\n",
    "print(\"Total registros:\", df_raw.count())"
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5842fe37-8b8e-43a6-9f14-7dc16eae08a2",
     "showTitle": false,
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
     "tableResultSettingsMap": {},
=======
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"event_ts\":217},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753221646008}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 4: Spatial‐join con UDF (Shapely) — con district y neighborhood\n",
    "# ==============================\n",
    "from shapely.geometry import Point, shape\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# UDF que asigna el código de barrio (district) según lat/lon\n",
    "def find_district(lat, lon):\n",
    "    pt = Point(lon, lat)\n",
    "    for b in neigh_list:\n",
    "        if shape(b[\"geometry\"]).contains(pt):\n",
    "            return b[\"IDENTIFICACION\"]\n",
    "    return None\n",
    "\n",
    "find_district_udf = udf(find_district, StringType())\n",
    "\n",
    "# UDF que asigna el nombre de barrio (neighborhood) según lat/lon\n",
    "def find_neighborhood_name(lat, lon):\n",
    "    pt = Point(lon, lat)\n",
    "    for b in neigh_list:\n",
    "        if shape(b[\"geometry\"]).contains(pt):\n",
    "            return b[\"NOMBRE\"]\n",
    "    return None\n",
    "\n",
    "find_name_udf = udf(find_neighborhood_name, StringType())\n",
    "\n",
    "# Aplicamos ambos UDFs para enriquecer df_raw\n",
    "df_events = (\n",
    "    df_raw\n",
    "      .withColumn(\"district\", find_district_udf(\"latitude\", \"longitude\"))\n",
    "      .withColumn(\"neighborhood\", find_name_udf(\"latitude\", \"longitude\"))\n",
    "      .drop(\"neigh_id\")\n",
    ")\n",
    "\n",
    "# Verificamos resultado\n",
    "print(\"✅ Spatial‐join completo: código y nombre de barrio asignados\")\n",
    "df_events.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc9b3cd-f146-45cc-a93f-e073a301abe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 5: Transformar a esquema Bronze con columnas reordenadas\n",
    "# ==============================\n",
    "from pyspark.sql.functions import (\n",
    "    to_timestamp, date_format,\n",
    "    year, month, dayofmonth, hour, minute, second,\n",
    "    col\n",
    ")\n",
    "\n",
    "df_bronze = (\n",
    "    df_events\n",
    "      # Convertir la fecha string a timestamp para extraer componentes\n",
    "      .withColumn(\"event_ts\", to_timestamp(\"date\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "      # Formatear partition_date como ddMMyyyy\n",
    "      .withColumn(\"partition_date\", date_format(\"event_ts\", \"ddMMyyyy\"))\n",
    "      # Desglosar componentes de fecha/hora\n",
    "      .withColumn(\"event_year\",  year(\"event_ts\"))\n",
    "      .withColumn(\"event_month\", month(\"event_ts\"))\n",
    "      .withColumn(\"event_day\",   dayofmonth(\"event_ts\"))\n",
    "      .withColumn(\"event_hour\",  hour(\"event_ts\"))\n",
    "      .withColumn(\"event_minute\", minute(\"event_ts\"))\n",
    "      .withColumn(\"event_second\", second(\"event_ts\"))\n",
    "      # Selección final en el orden deseado\n",
    "      .select(\n",
    "         \"partition_date\",\n",
    "         \"order_id\",\n",
    "         \"neighborhood\",\n",
    "         \"customer_id\",\n",
    "         \"employee_id\",\n",
    "         col(\"date\").alias(\"event_date\"),\n",
    "         \"event_day\",\n",
    "         \"event_hour\",\n",
    "         \"event_minute\",\n",
    "         \"event_month\",\n",
    "         \"event_second\",\n",
    "         \"event_year\",\n",
    "         \"latitude\",\n",
    "         \"longitude\",\n",
    "         \"district\",\n",
    "         \"quantity_products\"\n",
    "      )\n",
    ")\n",
    "\n",
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "# Verificar y mostrar\n",
    "print(\"✅ Bronze reordenado según el esquema original\")\n",
    "df_bronze.printSchema()\n",
    "df_bronze.show(5, truncate=False)\n"
=======
    "# Inspección rápida del resultado\n",
    "# display(df_bronze.limit(5))\n",
    "df_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07ace30-f93c-404d-ab19-0ef6c10455ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Celda X: Crear el schema unalwater\n",
    "CREATE DATABASE IF NOT EXISTS unalwater;\n",
    "\n",
    "-- Celda X: Crear el schema de prueba\n",
    "CREATE DATABASE IF NOT EXISTS poctesting;"
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d90bf64-59d4-4d20-8229-c4fcbaf75705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 6: Persistir en Delta y verificar\n",
    "# ==============================\n",
    "# Crea la base si no existe\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS unalwater\")\n",
    "\n",
    "(\n",
    "  df_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "    .partitionBy(\"partition_date\")\n",
    "    .saveAsTable(\"unalwater.bronze_events\")\n",
    ")\n",
    "\n",
    "# Verificación\n",
    "total = spark.table(\"unalwater.bronze_events\").count()\n",
    "print(f\"✅ Bronze listo. Total registros en tabla: {total}\")\n",
    "spark.table(\"unalwater.bronze_events\").show(5, truncate=False)\n"
=======
    "    .saveAsTable(\"poctesting.bronze_events\")\n",
    ")\n",
    "\n",
    "# 2) Verificar conteo final\n",
    "total = spark.table(\"poctesting.bronze_events\").count()\n",
    "print(f\"✅ Bronze listo. Total registros: {total}\")\n",
    "# spark.table(\"unalwater.bronze_events\").show(5, truncate=False)\n"
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cac943d-ac33-4ae0-8773-e7a2d88f82b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
    "\n",
    "SELECT * \n",
    "FROM unalwater.bronze_events\n",
    "LIMIT 21;"
=======
    "-- DESCRIBE DETAIL unalwater.bronze_events;\n",
    "-- SELECT DISTINCT partition_date FROM unalwater.bronze_events ORDER BY partition_date;\n",
    "-- SELECT * FROM unalwater.bronze_events WHERE partition_date = '22072025' LIMIT 5;\n"
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "geopandas",
     "pyarrow",
     "numpy==1.24.4",
     "pandas==2.0.3",
     "scipy==1.11.1"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
<<<<<<< Updated upstream:notebooks/1_simulation/01_setup_and_load.ipynb
     "commandId": 8267139422616542,
=======
     "commandId": 8224597185425527,
>>>>>>> Stashed changes:notebooks/1_Ingestion/01_setup_and_load.ipynb
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_setup_and_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

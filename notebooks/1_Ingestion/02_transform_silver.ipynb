{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cddcedf-0e32-4eb6-8040-3c352a1abd30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09016fed-ec3a-47a1-b9f4-5704c6a8f693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Traemos los datos de prueba bronces\n",
    "# data = spark.table(\"poctesting.bronze_events\")\n",
    "# display(data.limit(5))\n",
    "# print(f\"El archivo tiene {data.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206d945b-a866-4105-8279-c412e933cadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Importar bronze\n",
    "df_bronze = spark.table(\"poctesting.bronze_events\")\n",
    "\n",
    "# Registros completos\n",
    "df_completos = df_bronze.filter(\n",
    "    col(\"neighborhood\").isNotNull() & col(\"district\").isNotNull()\n",
    ")\n",
    "\n",
    "# Registros incompletos\n",
    "df_incompletos = df_bronze.filter(\n",
    "    col(\"neighborhood\").isNull() | col(\"district\").isNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4298d0dc-7874-40f7-a767-75a634d58dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, count\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import BallTree\n",
    "import numpy as np\n",
    "\n",
    "def corregir_con_sjoin_y_balltree(df_spark, path_parquet_neigh, schema):\n",
    "    df = df_spark.toPandas()\n",
    "    if df.empty:\n",
    "        return spark.createDataFrame([], schema)\n",
    "\n",
    "    df[\"geometry\"] = df.apply(lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    gdf_neigh = gpd.read_parquet(path_parquet_neigh)[[\"NOMBRE\", \"IDENTIFICACION\", \"geometry\"]]\n",
    "    gdf_neigh = gdf_neigh.dropna(subset=[\"geometry\"]).set_crs(\"EPSG:4326\")\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "    gdf_neigh = gdf_neigh.to_crs(\"EPSG:3857\")\n",
    "\n",
    "    gdf_joined = gpd.sjoin(gdf, gdf_neigh, how=\"left\", predicate=\"within\")\n",
    "    gdf[\"neighborhood\"] = gdf_joined[\"NOMBRE\"]\n",
    "    gdf[\"district\"] = gdf_joined[\"IDENTIFICACION\"]\n",
    "\n",
    "    gdf_nulos = gdf[gdf[\"neighborhood\"].isna()].copy()\n",
    "    if not gdf_nulos.empty:\n",
    "        centroids = gdf_neigh.geometry.centroid\n",
    "        neigh_coords = np.array([[pt.y, pt.x] for pt in centroids])\n",
    "        point_coords = np.array([[pt.y, pt.x] for pt in gdf_nulos.geometry])\n",
    "        tree = BallTree(np.deg2rad(neigh_coords), metric=\"haversine\")\n",
    "        dist, idx = tree.query(np.deg2rad(point_coords), k=1)\n",
    "        gdf.loc[gdf[\"neighborhood\"].isna(), \"neighborhood\"] = gdf_neigh.iloc[idx.flatten()][\"NOMBRE\"].values\n",
    "        gdf.loc[gdf[\"district\"].isna(), \"district\"] = gdf_neigh.iloc[idx.flatten()][\"IDENTIFICACION\"].values\n",
    "\n",
    "    gdf = gdf.dropna(subset=[\"neighborhood\", \"district\"])\n",
    "\n",
    "    if gdf.empty:\n",
    "        return spark.createDataFrame([], schema)\n",
    "    else:\n",
    "        return spark.createDataFrame(gdf.drop(columns=[\"geometry\"]))\n",
    "\n",
    "def actualizar_silver_eventos(df_completos, df_incompletos, path_parquet_neigh):\n",
    "    nombre_tabla_silver = \"poctesting.silver_events\"\n",
    "    schema = df_incompletos.schema\n",
    "\n",
    "    # Corregir los incompletos\n",
    "    df_corregido = corregir_con_sjoin_y_balltree(df_incompletos, path_parquet_neigh, schema)\n",
    "    cantidad_corregidos = df_corregido.count()\n",
    "    cantidad_completos = df_completos.count()\n",
    "\n",
    "    # Unir completados\n",
    "    if df_corregido.limit(1).count() == 0:\n",
    "        print(f\"⚠️ No se corrigieron registros incompletos. Se usará solo df_completos ({cantidad_completos}).\")\n",
    "        df_union = df_completos\n",
    "    else:\n",
    "        print(f\"✅ Se corrigieron {cantidad_corregidos} registros. Total con completos: {cantidad_completos + cantidad_corregidos}\")\n",
    "        df_union = df_completos.unionByName(df_corregido)\n",
    "\n",
    "    # Eliminar duplicados por order_id conservando el de mayor quantity_products\n",
    "    window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"quantity_products\").desc())\n",
    "    df_union_dedup = df_union.withColumn(\"rn\", row_number().over(window_spec)).filter(\"rn = 1\").drop(\"rn\")\n",
    "\n",
    "    # Cargar datos existentes (si los hay)\n",
    "    tabla_existe = spark.catalog.tableExists(nombre_tabla_silver)\n",
    "    if tabla_existe:\n",
    "        df_existente = spark.table(nombre_tabla_silver)\n",
    "        df_todo = df_existente.unionByName(df_union_dedup)\n",
    "\n",
    "        # Deduplicar final por order_id (mantener mayor quantity_products)\n",
    "        window_final = Window.partitionBy(\"order_id\").orderBy(col(\"quantity_products\").desc())\n",
    "        df_final = df_todo.withColumn(\"rn\", row_number().over(window_final)).filter(\"rn = 1\").drop(\"rn\")\n",
    "    else:\n",
    "        df_final = df_union_dedup\n",
    "\n",
    "    # Guardar en tabla silver\n",
    "    modo = \"overwrite\" if not tabla_existe else \"overwrite\"\n",
    "    df_final.write.mode(modo).saveAsTable(nombre_tabla_silver)\n",
    "\n",
    "    # Verificación final de duplicados\n",
    "    df_verif = spark.table(nombre_tabla_silver)\n",
    "    df_check = df_verif.groupBy(\"order_id\").agg(count(\"*\").alias(\"cantidad\")).filter(\"cantidad > 1\")\n",
    "\n",
    "    if df_check.count() > 0:\n",
    "        print(f\"❌ Duplicados encontrados en 'order_id': {df_check.count()}\")\n",
    "        display(df_check)\n",
    "    else:\n",
    "        print(f\"✅ Tabla 'silver_eventos' actualizada correctamente con {df_final.count()} registros únicos por orden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba35b07-63c0-43a8-8596-fd4259a8d19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_parquet_neigh = \"/Workspace/Users/danielale22rojas@gmail.com//medellin-bigdata-poc/data/raw/medellin_neighborhoods.parquet\"\n",
    "actualizar_silver_eventos(df_completos, df_incompletos, path_parquet_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309b819f-9d69-4b92-911a-dff07f1d4ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.table(\"poctesting.silver_events\")\n",
    "# df_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"poctesting.silver_events\")\n",
    "print(f\"Descartamos {df_bronze.count() - df_silver.count()} registros con información faltante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df263bcd-38b8-45b6-b2c8-9421094ea9b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- cual es el numero promedio de produtos vendidos\n",
    "-- SELECT avg(quantity_products) FROM poctesting.silver_events;\n",
    "-- SELECT avg(quantity_products) FROM poctesting.bronze_events;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "geopandas",
     "pyarrow",
     "numpy==1.24.4",
     "pandas==2.0.3",
     "scipy==1.11.1",
     "folium",
     "shapely",
     "pointpats",
     "mapclassify"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5790031411786163,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_transform_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cddcedf-0e32-4eb6-8040-3c352a1abd30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6c0b07-62e3-43c4-9649-cc16cfd88b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for i in range(1000):\n",
    "    print(f\"▶️ Ejecutando iteración {i+1}\")\n",
    "    dbutils.notebook.run(\"./setup_optimize\", timeout_seconds=200)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2355ee6a-08ce-430a-aa48-7a15e1d58973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run ./setup_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09016fed-ec3a-47a1-b9f4-5704c6a8f693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Traemos los datos de prueba bronces después de recopilar una hora de simulación\n",
    "data = spark.table(\"poctesting.bronze_events\")\n",
    "display(data.limit(5))\n",
    "print(f\"El archivo tiene {data.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206d945b-a866-4105-8279-c412e933cadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Importar bronze\n",
    "df_bronze = spark.table(\"poctesting.bronze_events\")\n",
    "\n",
    "# Registros completos\n",
    "df_completos = df_bronze.filter(\n",
    "    col(\"neighborhood\").isNotNull() & col(\"district\").isNotNull()\n",
    ")\n",
    "\n",
    "# Registros incompletos\n",
    "df_incompletos = df_bronze.filter(\n",
    "    col(\"neighborhood\").isNull() | col(\"district\").isNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28deadc-caec-4d07-9718-169c2bcabdd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.count()\n",
    "# df_incompletos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4298d0dc-7874-40f7-a767-75a634d58dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, count\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import BallTree\n",
    "import numpy as np\n",
    "\n",
    "def corregir_con_sjoin_y_balltree(df_spark, path_parquet_neigh, schema):\n",
    "    df = df_spark.toPandas()\n",
    "    if df.empty:\n",
    "        return spark.createDataFrame([], schema)\n",
    "\n",
    "    df[\"geometry\"] = df.apply(lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    gdf_neigh = gpd.read_parquet(path_parquet_neigh)[[\"NOMBRE\", \"IDENTIFICACION\", \"geometry\"]]\n",
    "    gdf_neigh = gdf_neigh.dropna(subset=[\"geometry\"]).set_crs(\"EPSG:4326\")\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "    gdf_neigh = gdf_neigh.to_crs(\"EPSG:3857\")\n",
    "\n",
    "    gdf_joined = gpd.sjoin(gdf, gdf_neigh, how=\"left\", predicate=\"within\")\n",
    "    gdf[\"neighborhood\"] = gdf_joined[\"NOMBRE\"]\n",
    "    gdf[\"district\"] = gdf_joined[\"IDENTIFICACION\"]\n",
    "\n",
    "    gdf_nulos = gdf[gdf[\"neighborhood\"].isna()].copy()\n",
    "    if not gdf_nulos.empty:\n",
    "        centroids = gdf_neigh.geometry.centroid\n",
    "        neigh_coords = np.array([[pt.y, pt.x] for pt in centroids])\n",
    "        point_coords = np.array([[pt.y, pt.x] for pt in gdf_nulos.geometry])\n",
    "        tree = BallTree(np.deg2rad(neigh_coords), metric=\"haversine\")\n",
    "        dist, idx = tree.query(np.deg2rad(point_coords), k=1)\n",
    "        gdf.loc[gdf[\"neighborhood\"].isna(), \"neighborhood\"] = gdf_neigh.iloc[idx.flatten()][\"NOMBRE\"].values\n",
    "        gdf.loc[gdf[\"district\"].isna(), \"district\"] = gdf_neigh.iloc[idx.flatten()][\"IDENTIFICACION\"].values\n",
    "\n",
    "    gdf = gdf.dropna(subset=[\"neighborhood\", \"district\"])\n",
    "\n",
    "    if gdf.empty:\n",
    "        return spark.createDataFrame([], schema)\n",
    "    else:\n",
    "        return spark.createDataFrame(gdf.drop(columns=[\"geometry\"]))\n",
    "\n",
    "def actualizar_silver_eventos(df_completos, df_incompletos, path_parquet_neigh):\n",
    "    nombre_tabla_silver = \"silver_eventos\"\n",
    "    schema = df_incompletos.schema\n",
    "\n",
    "    # Corregir los incompletos\n",
    "    df_corregido = corregir_con_sjoin_y_balltree(df_incompletos, path_parquet_neigh, schema)\n",
    "    cantidad_corregidos = df_corregido.count()\n",
    "    cantidad_completos = df_completos.count()\n",
    "\n",
    "    # Unir completados\n",
    "    if df_corregido.limit(1).count() == 0:\n",
    "        print(f\"⚠️ No se corrigieron registros incompletos. Se usará solo df_completos ({cantidad_completos}).\")\n",
    "        df_union = df_completos\n",
    "    else:\n",
    "        print(f\"✅ Se corrigieron {cantidad_corregidos} registros. Total con completos: {cantidad_completos + cantidad_corregidos}\")\n",
    "        df_union = df_completos.unionByName(df_corregido)\n",
    "\n",
    "    # Eliminar duplicados por order_id conservando el de mayor quantity_products\n",
    "    window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"quantity_products\").desc())\n",
    "    df_union_dedup = df_union.withColumn(\"rn\", row_number().over(window_spec)).filter(\"rn = 1\").drop(\"rn\")\n",
    "\n",
    "    # Cargar datos existentes (si los hay)\n",
    "    tabla_existe = spark.catalog.tableExists(nombre_tabla_silver)\n",
    "    if tabla_existe:\n",
    "        df_existente = spark.table(nombre_tabla_silver)\n",
    "        df_todo = df_existente.unionByName(df_union_dedup)\n",
    "\n",
    "        # Deduplicar final por order_id (mantener mayor quantity_products)\n",
    "        window_final = Window.partitionBy(\"order_id\").orderBy(col(\"quantity_products\").desc())\n",
    "        df_final = df_todo.withColumn(\"rn\", row_number().over(window_final)).filter(\"rn = 1\").drop(\"rn\")\n",
    "    else:\n",
    "        df_final = df_union_dedup\n",
    "\n",
    "    # Guardar en tabla silver\n",
    "    modo = \"overwrite\" if not tabla_existe else \"overwrite\"\n",
    "    df_final.write.mode(modo).saveAsTable(nombre_tabla_silver)\n",
    "\n",
    "    # Verificación final de duplicados\n",
    "    df_verif = spark.table(nombre_tabla_silver)\n",
    "    df_check = df_verif.groupBy(\"order_id\").agg(count(\"*\").alias(\"cantidad\")).filter(\"cantidad > 1\")\n",
    "\n",
    "    if df_check.count() > 0:\n",
    "        print(f\"❌ Duplicados encontrados en 'order_id': {df_check.count()}\")\n",
    "        display(df_check)\n",
    "    else:\n",
    "        print(f\"✅ Tabla 'silver_eventos' actualizada correctamente con {df_final.count()} registros únicos por orden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba35b07-63c0-43a8-8596-fd4259a8d19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_parquet_neigh = \"/Workspace/Users/danielale22rojas@gmail.com//medellin-bigdata-poc/data/raw/medellin_neighborhoods.parquet\"\n",
    "actualizar_silver_eventos(df_completos, df_incompletos, path_parquet_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309b819f-9d69-4b92-911a-dff07f1d4ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.table(\"silver_eventos\")\n",
    "# silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"poctesting.silver_events\")\n",
    "print(f\"Descartamos {df_bronze.count() - df_silver.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29429ca2-889a-484d-b45d-81bd473c65e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ventanas para Gold\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Definir ventanas\n",
    "window_neigh = Window.partitionBy(\"neighborhood\")\n",
    "\n",
    "# Agregar columnas con totales\n",
    "df_gold = df_silver \\\n",
    "    .withColumn(\"total_by_neighborhood\", F.sum(\"quantity_products\").over(window_neigh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c62002-5ab4-4181-9d50-dfe26d48678e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_gold.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2a2fcd-44e9-4991-9ae0-00abebe249b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet(path_parquet_neigh) \n",
    "gdf[\"NOMBRE\"] = gdf[\"NOMBRE\"].str.replace(\"CORREGIMIENTO DE \", \"\", regex=True)\n",
    "gdf[\"NOMBRE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f096e05-74f0-434f-b282-906bc977c823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Cargar los polígonos de los barrios\n",
    "gdf = gpd.read_parquet(path_parquet_neigh) \n",
    "gdf = gdf[[\"NOMBRE\", \"geometry\"]]\n",
    "\n",
    "# 2. Leer Gold y dejar un registro por barrio\n",
    "pdf_gold = df_gold.select(\"neighborhood\", \"total_by_neighborhood\").distinct().toPandas()\n",
    "\n",
    "# 3. Unir con datos espaciales\n",
    "gdf_merged = gdf.merge(pdf_gold, left_on=\"NOMBRE\", right_on=\"neighborhood\", how=\"left\")\n",
    "gdf_merged[\"NOMBRE\"] = gdf_merged[\"NOMBRE\"].str.replace(\"CORREGIMIENTO DE \", \"\", regex=True)\n",
    "\n",
    "# 4. Crear gráfico\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "gdf_merged.plot(\n",
    "    column=\"total_by_neighborhood\",\n",
    "    cmap=\"OrRd\",\n",
    "    edgecolor=\"black\",\n",
    "    legend=True,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# 5. Rotular barrios usando centroide\n",
    "for idx, row in gdf_merged.iterrows():\n",
    "    if row[\"geometry\"] is not None:\n",
    "        centroid = row[\"geometry\"].centroid\n",
    "        plt.text(\n",
    "            centroid.x,\n",
    "            centroid.y,\n",
    "            row[\"NOMBRE\"],\n",
    "            fontsize=6,\n",
    "            ha=\"center\",\n",
    "            va=\"center\"\n",
    "        )\n",
    "\n",
    "plt.title(\"Total de Productos por Barrio - Medellín\", fontsize=15)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Quitar la palabra corregimiento de los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38681de3-e2f2-484e-9410-6ceb51418ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf = gdf.rename(columns={\"NOMBRE\": \"neighborhood\"})\n",
    "gdf[\"neighborhood\"] = gdf[\"neighborhood\"].astype(str)\n",
    "\n",
    "\n",
    "gdf_merged = gdf.merge(\n",
    "    pdf_gold,\n",
    "    on=\"neighborhood\",\n",
    "    how=\"left\"\n",
    ").to_crs(epsg=4326).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Cargar polígonos\n",
    "gdf = gpd.read_parquet(base/paths[\"neighborhoods\"])[[\"NOMBRE\", \"geometry\"]]\n",
    "gdf = gdf.rename(columns={\"NOMBRE\": \"neighborhood\"})\n",
    "gdf[\"neighborhood\"] = gdf[\"neighborhood\"].astype(str)\n",
    "\n",
    "# 2. Leer gold y limpiar\n",
    "pdf_gold = df_gold.select(\"neighborhood\", \"total_by_neighborhood\").distinct().toPandas()\n",
    "pdf_gold[\"neighborhood\"] = pdf_gold[\"neighborhood\"].astype(str)\n",
    "\n",
    "# 3. Unir y preparar\n",
    "gdf_merged = gdf.merge(pdf_gold, on=\"neighborhood\", how=\"left\")\n",
    "gdf_merged = gdf_merged.to_crs(epsg=4326).reset_index(drop=True)\n",
    "\n",
    "# 4. Crear mapa\n",
    "m = folium.Map(location=[6.25184, -75.56359], zoom_start=12)\n",
    "\n",
    "# 5. Choropleth asegurando que neighborhood esté en GeoJSON\n",
    "folium.Choropleth(\n",
    "    geo_data=gdf_merged.__geo_interface__,  # ← acceso directo a GeoJSON\n",
    "    data=pdf_gold,\n",
    "    columns=[\"neighborhood\", \"total_by_neighborhood\"],\n",
    "    key_on=\"feature.properties.neighborhood\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name=\"Total de Productos por Barrio\"\n",
    ").add_to(m)\n",
    "\n",
    "# 6. Popups\n",
    "for _, row in gdf_merged.iterrows():\n",
    "    if pd.notna(row[\"total_by_neighborhood\"]):\n",
    "        folium.Marker(\n",
    "            location=[row[\"geometry\"].centroid.y, row[\"geometry\"].centroid.x],\n",
    "            popup=f\"{row['neighborhood']}<br>Total: {int(row['total_by_neighborhood'])}\",\n",
    "            icon=folium.Icon(color='blue', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "\n",
    "# 7. Guardar el HTML\n",
    "m.save(\"/Workspace/Users/danielale22rojas@gmail/medellin-bigdata-poc/mapa/medellin_interactivo.html\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "geopandas",
     "pyarrow",
     "numpy==1.24.4",
     "pandas==2.0.3",
     "scipy==1.11.1",
     "folium"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7951172674825732,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_transform_and_compile",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

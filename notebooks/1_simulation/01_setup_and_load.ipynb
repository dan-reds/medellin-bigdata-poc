{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43319ef-6685-48e1-ba85-70a9f66457e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 1: Imports & Config\n",
    "import json, random, uuid, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "cfg      = json.loads(Path(\"/Workspace/Users/santiagobustosp@gmail.com/medellin-bigdata-poc/notebooks/1_simulation/sim_config.json\").read_text())\n",
    "base     = Path(cfg[\"base_path\"])\n",
    "paths    = cfg[\"paths\"]\n",
    "interval = cfg[\"interval_seconds\"]\n",
    "qty_min, qty_max = cfg[\"quantity_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2e09f6-ed4b-41ba-b807-1a0e9de52d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 2: Carga de insumos\n",
    "gdf_neigh = gpd.read_parquet(base/paths[\"neighborhoods\"])       # barrios\n",
    "mask_geom = gpd.read_parquet(base/paths[\"city_mask\"]).geometry.iloc[0]  # contorno Medellín\n",
    "df_cust   = pd.read_parquet(base/paths[\"customers\"])            # clientes\n",
    "df_emp    = pd.read_parquet(base/paths[\"employees\"])            # empleados\n",
    "print(f\"✅ Barrios: {len(gdf_neigh)} | Clientes: {len(df_cust)} | Empleados: {len(df_emp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd34b878-1d63-46a4-a75f-e358963bb7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 3: Funciones de muestreo y generación de evento\n",
    "def sample_point(poly):\n",
    "    minx,miny,maxx,maxy = poly.bounds\n",
    "    while True:\n",
    "        p = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "        if poly.contains(p) and mask_geom.contains(p):\n",
    "            return p\n",
    "\n",
    "def gen_event():\n",
    "    b  = gdf_neigh.sample(1).iloc[0]\n",
    "    pt = sample_point(b.geometry)\n",
    "    return {\n",
    "      \"order_id\":          str(uuid.uuid4()),\n",
    "      \"date\":              datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "      \"customer_id\":       int(df_cust.customer_id.sample(1).iloc[0]),\n",
    "      \"employee_id\":       int(df_emp.employee_id.sample(1).iloc[0]),\n",
    "      \"quantity_products\": random.randint(qty_min, qty_max),\n",
    "      \"latitude\":          pt.y,\n",
    "      \"longitude\":         pt.x,\n",
    "      \"neighborhood\":      b[\"IDENTIFICACION\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577287d6-6716-4814-8746-3040ad7b8b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 4: Preparar carpeta timestamp\n",
    "ts      = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = base/paths[\"output_dir\"]/ts\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"▶️ Carpeta de simulación:\", out_dir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b190138-38f1-46c4-9882-ee2e89e1dad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 5 optimizada para prueba rápida:\n",
    "N = 20\n",
    "for _ in range(N):\n",
    "    e = gen_event()\n",
    "    (out_dir/f\"{e['order_id']}.json\").write_text(json.dumps(e))\n",
    "print(f\"✅ Generados {N} eventos en {out_dir.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46aaf3d4-ef59-4565-83ec-00bdf4095f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 6: Leer los JSONs y crear DataFrame Spark\n",
    "files   = list(out_dir.glob(\"*.json\"))\n",
    "events  = [json.loads(p.read_text()) for p in files]\n",
    "df_raw  = spark.createDataFrame(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cec9aed-181d-4bee-8851-08b11ae3df91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 7: Inspección rápida\n",
    "display(df_raw)\n",
    "df_raw.printSchema()\n",
    "print(\"Total registros:\", df_raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b902eefd-ac2e-4046-90f3-0c485e112868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Geo‑join para calcular 'district'  \n",
    "from shapely.geometry import Point\n",
    "\n",
    "# 1) Convertir df_raw a pandas para hacer spatial join\n",
    "pdf = df_raw.toPandas()\n",
    "pdf = df_raw.toPandas().drop(columns=[\"neighborhood\"])\n",
    "\n",
    "# 2) Crear GeoDataFrame puntual con lat/lon\n",
    "pdf[\"geometry\"] = pdf.apply(lambda r: Point(r.longitude, r.latitude), axis=1)\n",
    "gpdf = gpd.GeoDataFrame(pdf, geometry=\"geometry\", crs=gdf_neigh.crs)\n",
    "\n",
    "# 3) Spatial join: cada punto recibe el polígono que lo contiene\n",
    "#    Suponemos gdf_neigh tiene columna 'NOMBRE' con el barrio\n",
    "gpdf = gpd.sjoin(gpdf, gdf_neigh[[\"IDENTIFICACION\",\"NOMBRE\", \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "\n",
    "# 4) Renombrar la columna resultante y limpiar índices\n",
    "gpdf = gpdf.rename(columns={\"IDENTIFICACION\": \"district\", \"NOMBRE\": \"neighborhood\"}).drop(columns=[\"index_right\"])\n",
    "\n",
    "# 5) Volver a Spark\n",
    "df_raw = spark.createDataFrame(gpdf.drop(columns=\"geometry\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9715c3f9-aae4-4ab8-b8ed-b109f012d7df",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752804737700}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 7: Inspección rápida\n",
    "display(df_raw)\n",
    "df_raw.printSchema()\n",
    "print(\"Total registros:\", df_raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6857a31-c7de-47d6-ab43-f611bbc49f1c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752804207900}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 8: Transformar df_raw al esquema Bronze\n",
    "from pyspark.sql.functions import (\n",
    "    to_timestamp, date_format,\n",
    "    year, month, dayofmonth,\n",
    "    hour, minute, second\n",
    ")\n",
    "\n",
    "df_bronze = (\n",
    "    df_raw\n",
    "      # 1) Parsear timestamp\n",
    "      .withColumn(\"event_ts\", to_timestamp(\"date\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "      # 2) Partición diaria en formato ddMMyyyy\n",
    "      .withColumn(\"partition_date\", date_format(\"event_ts\", \"ddMMyyyy\"))\n",
    "      # 3) Desglosar fecha/hora\n",
    "      .withColumn(\"event_year\",   year(\"event_ts\"))\n",
    "      .withColumn(\"event_month\",  month(\"event_ts\"))\n",
    "      .withColumn(\"event_day\",    dayofmonth(\"event_ts\"))\n",
    "      .withColumn(\"event_hour\",   hour(\"event_ts\"))\n",
    "      .withColumn(\"event_minute\", minute(\"event_ts\"))\n",
    "      .withColumn(\"event_second\", second(\"event_ts\"))\n",
    "      # 4) Renombrar/seleccionar columnas según spec\n",
    "      .select(\n",
    "         \"partition_date\",\n",
    "         \"order_id\",\n",
    "         \"neighborhood\",\n",
    "         \"customer_id\",\n",
    "         \"employee_id\",\n",
    "         \"event_ts\",\n",
    "         \"event_year\",\"event_month\",\"event_day\",\n",
    "         \"event_hour\",\"event_minute\",\"event_second\",\n",
    "         \"latitude\",\"longitude\",\n",
    "         \"district\",\n",
    "         \"quantity_products\"\n",
    "      )\n",
    ")\n",
    "\n",
    "# Inspección rápida del resultado\n",
    "display(df_bronze.limit(5))\n",
    "df_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07ace30-f93c-404d-ab19-0ef6c10455ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Celda X: Crear el schema unalwater\n",
    "CREATE DATABASE IF NOT EXISTS unalwater;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd849d1-d135-4358-beed-96fa3293b957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PASO 9: Persistir en Delta como managed table\n",
    "# Si la tabla no existe, la crea; si existe, hace append.\n",
    "\n",
    "# 1) Guardar como tabla delta en el metastore\n",
    "(\n",
    "  df_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"unalwater.bronze_events\")\n",
    ")\n",
    "\n",
    "# 2) Verificar conteo final\n",
    "total = spark.table(\"unalwater.bronze_events\").count()\n",
    "print(f\"✅ Bronze listo. Total registros: {total}\")\n",
    "spark.table(\"unalwater.bronze_events\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f7e769-09a0-4f98-9d61-90a7e43cc7e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE DETAIL unalwater.bronze_events;\n",
    "SELECT DISTINCT partition_date FROM unalwater.bronze_events ORDER BY partition_date;\n",
    "SELECT * FROM unalwater.bronze_events WHERE partition_date = '18072025' LIMIT 5;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8922762051288876,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_setup_and_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
